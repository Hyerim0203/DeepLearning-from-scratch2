{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "function_path = os.path.join(\"..\",\"deep-learning-from-scratch-2-master\")\n",
    "chapter4_path = os.path.join(function_path, \"ch04\")\n",
    "sys.path.append(function_path)\n",
    "sys.path.append(chapter4_path)\n",
    "\n",
    "from common.layers import Embedding\n",
    "from negative_sampling_layer import NegativeSamplingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V,H).astype(\"f\")\n",
    "        W_out = 0.01* np.random.randn(V,H).astype(\"f\")\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.in_layers = []\n",
    "        \n",
    "        # 맥락개수만큼 in_layer개수 생성\n",
    "        for i in range(2*window_size):\n",
    "            layer = Embedding(W_in)\n",
    "            self.in_layers.append(layer)\n",
    "            \n",
    "        # loss layer 생성\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out,corpus,power=0.75,sample_size=5)\n",
    "        \n",
    "        # 모든 가중치와 기울기를 배열에 모은다.\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장\n",
    "        self.words_vecs = W_in\n",
    "        \n",
    "        def forward(self,contexts, target):\n",
    "            h=0\n",
    "            \n",
    "            # in_layer 계산(각 층 계산후 평균값으로 h)\n",
    "            for i, layer in enumerate(self,in_layers):\n",
    "                h+= layer.forward(contexts[:,i])\n",
    "            h *= 1/len(self.in_layers)\n",
    "            \n",
    "            # h를 이용하여 sample_size+1개의 뉴런에서의 loss 계싼\n",
    "            loss = self.ns_loss.forward(h,target)\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "        def backward(self, dout=1):\n",
    "            dout = self.ns_loss.backward(dout)\n",
    "            dout *= 1/len(self.in_layers)\n",
    "            for layer in self.in_layers:\n",
    "                layer.backward(dout)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 모델 학습 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 좋은 성능을 주는 하이퍼 파리미터\n",
    "    - window_size : 2~10\n",
    "    - 은닉층의 뉴런 수(분산 표현의 차원 수) : 50~100개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "function_path = os.path.join(\"..\",\"deep-learning-from-scratch-2-master\")\n",
    "chapter4_path = os.path.join(function_path, \"ch04\")\n",
    "sys.path.append(function_path)\n",
    "sys.path.append(chapter4_path)\n",
    "\n",
    "import numpy as np\n",
    "from common import config\n",
    "\n",
    "import pickle\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "\n",
    "from cbow import CBOW\n",
    "from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "from dataset import ptb\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data(\"train\")\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "if config.GPU:\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "    \n",
    "# 모델 등 생성\n",
    "model = CBOW(vocab_size, hidden_size , window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model,optimizer)\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "# 나중에 사용할 수 있도록 필요한 데이터 저장\n",
    "word_vecs = model.word_vecs # in_weight 꺼내오기\n",
    "if config.GPU:\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "params = {} # 필요한 파라미터 저장\n",
    "params[\"word_vecs\"] = word_vecs.astype(np.float16)\n",
    "params[\"word_to_id\"] = word_to_id\n",
    "params[\"id_to_word\"] = id_to_word\n",
    "\n",
    "pkl_file = \"cbow_params.pkl\" # 파일명\n",
    "\n",
    "# pkl_file 저장\n",
    "with open(pkl_file, \"wb\") as f:\n",
    "    pickle.dump(params, f,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *pickle*\n",
    "- list/class 같은 텍스트 형식이 아닌 자료형은 일반적인 방법으로 불러오거나 쓸 수 없음. -> pickle이 이를 제공\n",
    "- 모든 데이터 객체를 저장하고 읽을 수 있음\n",
    "- pickle로 데이터를 저장하거나 불러올때는 파일을 바이트형식으로 읽거나 써야함. (wb, rb)(원래는 w/r)\n",
    "- wb로 데이터를 입력하는 경우는 .bin 확장자를 사용하는게 좋음\n",
    "- 입력(pickle.dump(data, file))\n",
    "    - list = [1,2,3,4]\n",
    "    - with open('list.txt', 'wb') as f:  \n",
    "        pickle.dump(list, f)\n",
    "- 출력(pickle.load(file))\n",
    "    - 한줄씩 읽어오고 더이상 읽어올 데이터가 없으면 Error 발생\n",
    "    -  with open('list.txt', 'rb') as f:  \n",
    "        data = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = chapter4_path+\"/cbow_params.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " we: 0.6103515625\n",
      " someone: 0.59130859375\n",
      " i: 0.55419921875\n",
      " something: 0.48974609375\n",
      " anyone: 0.47314453125\n",
      "\n",
      "[query] year\n",
      " month: 0.71875\n",
      " week: 0.65234375\n",
      " spring: 0.62744140625\n",
      " summer: 0.6259765625\n",
      " decade: 0.603515625\n",
      "\n",
      "[query] car\n",
      " luxury: 0.497314453125\n",
      " arabia: 0.47802734375\n",
      " auto: 0.47119140625\n",
      " disk-drive: 0.450927734375\n",
      " travel: 0.4091796875\n",
      "\n",
      "[query] toyota\n",
      " ford: 0.55078125\n",
      " instrumentation: 0.509765625\n",
      " mazda: 0.49365234375\n",
      " bethlehem: 0.47509765625\n",
      " nissan: 0.474853515625\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from common.util import most_similar\n",
    "import pickle\n",
    "pkl_file = \"cbow_params.pkl\"\n",
    "\n",
    "with open(chapter4_path + \"/\"+pkl_file,\"rb\") as f:\n",
    "    params = pickle.load(f)\n",
    "    word_vecs = params[\"word_vecs\"]\n",
    "    word_to_id = params[\"word_to_id\"]\n",
    "    id_to_word = params[\"id_to_word\"]\n",
    "\n",
    "querys = [\"you\",\"year\",\"car\",\"toyota\"]\n",
    "for query in querys:\n",
    "    most_similar(query,word_to_id,id_to_word,word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec을 이용한 유추문제\n",
    "- word2vec으로 얻은 단어의 분산 표현에서 비슷한 단어는 벡터공간에서 가깝게 위치. 더 복잡한 패턴 또한 파악함\n",
    "- 벡터의 덧셈과 뺄셈을 이용하여 유추문제를 풀 수 있음\n",
    "    - 단어의 단순한 의미(왕,남자,왕비,여자)뿐 아니라 문법적인 패턴(복수형, 과거/현재/미래형, 비교형 등)도 알 수 있음\n",
    "- a:b=c:?\n",
    "    - b와 a의 차이를 c에 더해줌으로써 ?를 구함\n",
    "    - ? = b-a+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1))\n",
    "        x /= s.reshape((s.shape[0], 1))\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x /= s\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(a,b,c, word_to_id, id_to_word,word_matrix, top=5, answer=None):\n",
    "    for word in (a,b,c):\n",
    "        if word not in word_to_id:\n",
    "            print(\"%s(을)를 찾을 수 없습니다\"%word)\n",
    "            return\n",
    "    \n",
    "    # a:b=c:?\n",
    "    # b와 a의 차이를 c에 더해줌으로써 ?를 구함\n",
    "    print(\"\\n[analogy]\"+a+\":\"+b+\"=\"+c+\":?\")\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]],word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec-a_vec+c_vec\n",
    "    \n",
    "    # 크기가 1이 되도록 정규화 시켜줌\n",
    "    query_vec = normalize(query_vec)\n",
    "    #word_matrix = normalize(word_matrix)\n",
    "    \n",
    "    # 코싸인 유사도\n",
    "    similarity = np.dot(word_matrix,query_vec)\n",
    "    \n",
    "    if answer is not None:\n",
    "        print(\"==>\"+answer+\":\"+str(np.dot(word_matrix[word_to_id[answer]]), query_vec))\n",
    "        \n",
    "    count = 0\n",
    "    \n",
    "    # 유사도가 높은 것 순서대로 인덱싱 정렬\n",
    "    for i in (-1*similarity).argsort():\n",
    "        if np.isnan(similarity[i]): # nan 값이면\n",
    "            continue\n",
    "        if id_to_word[i] in (a,b,c): # a,b,c 안에 있는 단어면\n",
    "            continue\n",
    "        print(\"{0}:{1}\".format(id_to_word[i],similarity[i]))\n",
    "        \n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy]king:man=queen:?\n",
      "arabs:0.459716796875\n",
      "downside:0.455322265625\n",
      "predecessor:0.44384765625\n",
      "solution:0.434814453125\n",
      "woman:0.433349609375\n",
      "\n",
      "[analogy]take:took=go:?\n",
      "went:0.47412109375\n",
      "began:0.437255859375\n",
      "starts:0.414794921875\n",
      "turns:0.4130859375\n",
      "comes:0.41259765625\n",
      "\n",
      "[analogy]car:cars=child:?\n",
      "children:0.50732421875\n",
      "cattle:0.47412109375\n",
      "sand:0.446533203125\n",
      "screen:0.4375\n",
      "sports:0.437255859375\n",
      "\n",
      "[analogy]good:better=bad:?\n",
      "slower:0.495361328125\n",
      "more:0.48291015625\n",
      "less:0.4609375\n",
      "greater:0.432373046875\n",
      "wider:0.4150390625\n"
     ]
    }
   ],
   "source": [
    "analogy(\"king\",\"man\",\"queen\",word_to_id, id_to_word, word_vecs, top=5) \n",
    "analogy(\"take\",\"took\",\"go\",word_to_id, id_to_word, word_vecs, top=5)\n",
    "analogy(\"car\",\"cars\",\"child\",word_to_id, id_to_word, word_vecs, top=5)\n",
    "analogy(\"good\",\"better\",\"bad\",word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어의 분산 표현을 이용한 시스템 처리 흐름\n",
    "질문(자연어) -> 단어 벡터화(word2vec) -> 머신러닝 시스템(NN/SVM 등) -> 답변"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 단어 분산 표현 학습과 머신러닝 시스템의 학습은 서로 다른 데이터셋을 사용해 개별적으로 수행\n",
    "- 단어 벡터화\n",
    "    - 단어/문장을 고정 길이 벡터로 변환할 수 있다는 것은 머신러닝 시스템의 입력으로 사용할 수 있고, 답을 출력하는 것이 가능해진다는 점에서 중요\n",
    "    - 전이학습(한 분야에서 배운 지식을 다른 분야에도 적용하는 기법)을 이용\n",
    "        - 큰 말뭉치(위키백과/구글 뉴스의 텍스트 데이터 등)으로 학습을 마친 분산표현을 각자의 작업에 이용\n",
    "        - 이러한 단어의 분산표현을 이용하여 문장 또한 분산표현으로 나타낼 수 있음\n",
    "            - 가장 간단한 방법(bag of words) : 문장의 각 단어를 분산 표현으로 변환하고 그 합을 구함, 순서를 고려하지 않음\n",
    "            - RNN을 이용\n",
    "- 머신러닝 시스템\n",
    "    - 일반적으로 단어의 분산표현은 범용 말뭉치를 통해 미리 학습.\n",
    "    - 머신러닝 시스템 학습은 현재 직면한 문제(텍스트 분류/문서 클러스터링/품사 태그달기/감성 분석 등)에 관련하여 수집한 데이터로 학습시킴\n",
    "        - 직면한 문제의 학습 데이터가 아주 많다면 해당 데이터로 단어의 분산 표현, 머신러닝 시스템 학습 모두를 수행하는 방안 또한 고려해볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 벡터 평가 방법\n",
    "- 실제 애플리케이션을 고려해서 평가\n",
    "    - 자연어 처리 시스템은 단어분산표현과 머신러닝 시스템으로 구성\n",
    "    - 현실적 문제 해결 관점에서 평가할 수 있음\n",
    "    - but 두 시스템 모두를 고려하게 되면, 단어의 분산 표현의 차원에 따라 머신러닝 시스템의 최적의 하이퍼파라미터를 찾기 위한 튜닝도 필요하기 때문에, 실제 애플리케이션과는 분리해서 평가.\n",
    "- 실제 애플리케이션과 분리해서 평가(일반적)\n",
    "    - 평가 척도 : 유사성, 유추문제\n",
    "    - 유사성 : 사람이 작성한 단어 유사도를 검증 세트로 사용해 평가\n",
    "        - 사람이 부여한 점수와 word2vec에 의한 코사인 유사도 점수를 비교해 그 상관성을 봄\n",
    "    - 유추 문제 : 의미(semantics), 구문(syntax) 등 평가\n",
    "        - 모델에 따라 정확도가 다름(CBOW/skip-gram)\n",
    "            - 말뭉치에 따라 적합한 모델 선택(일반적으로 skip_gram 성능이 더 좋음.)\n",
    "        - 일반적으로 말뭉치가 클수록 결과가 좋음(항상 데이터가 많은 게 좋음)\n",
    "        - 단어 벡터 차원 수는 적당한 크기가 좋음(너무 커도 정확도가 나빠짐)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어 분산 표현의 우수함이 우수한 애플리케이션에 얼마나 기여하는지는 애플리케이션의 종류/말뭉치의 내용/다루는 문제 상황에 따라 다름\n",
    "- 유츄 문제에 의한 평가가 높으면 애플리케이션에서도 좋은 결과를 기대할 수 있겠지만, 반드시 좋은 결과가 나오리라는 보장은 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "- 컴퓨터가 모든 데이터를 처리하는 것은 비현실적 -> 꼭 필요한 일부에 집중할 수 있도록 해줌"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
