{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter2\n",
    "- 자연어처리 - 컴퓨터가 우리의 말을 이해하게 만드는 것\n",
    "- 특히 고전적인 방법을 중심으로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 자연어 처리란\n",
    "- 자연어 : 평소에 쓰는 말\n",
    "- 자연어 처리(NLP) : 자연어를 컴퓨터에게 이해시키기 위한 기술"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 단어의 의미\n",
    "- 단어 : 의미의 최소단위\n",
    "- 단어의 의미를 표현하는 방법\n",
    "    - 시소러스를 활용한 기법\n",
    "    - 통계기반기법\n",
    "    - 추론기반기법(word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 시소러스를 이용한 방법\n",
    "- 시소러스를 이용하여 컴퓨터가 단어의 의미를 이해하도록 하는 것\n",
    "- 시소러스 : 유의어 사전으로, 뜻이 같은 단어(동의어)/뜻이 비슷한 단어(유의어)가 한 그룹으로 분류되어 있음  \n",
    "    - 사람이 만듦(사람이 단어의 특성을 직접 추출하는 방법)  \n",
    "ex) car = auto, automobile, machine, motorcar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 유의어 집합을 만든 후, 상/하위 or 전체와 부분과 같이 더욱 세세한 관계까지 정의하기도 함\n",
    "- 상하위관계\n",
    "    - object\n",
    "        - motor vehicle(동력차)\n",
    "            - car\n",
    "                - suv\n",
    "                - compact\n",
    "                - hatch-back\n",
    "            - go-kart\n",
    "            - truck\n",
    "- 이러한 단어들의 관계 그래프인 **단어 네트워크**를 이용하면 컴퓨터에 단어의 의미를 가르칠 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 WordNet\n",
    "- 가장 유명한 시소러스 : WordNet\n",
    "- WordNet : 유의어, 단어 네트워크를 이용할 수 있음\n",
    "    - 단어 네트워크를 이용하여 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 시소러스의 문제점\n",
    "1. 시대변화에 대응하기 어려움\n",
    "    - 단어는 생성되기도, 소멸하기도 함\n",
    "    - 시대에 따라 언어의 의미가 변하기도 함\n",
    "1. 사람을 쓰는 비용이 큼\n",
    "    - 단어의 수가 굉장히 많고, 이 모든 단어에 대한 관계를 정의하는 것은 어려움\n",
    "1. 단어의 미묘한 차이를 표현할 수 없음\n",
    "    - 유의어들을 한 그룹으로 묶기 때문에, 유의어일지라도 미묘한 차이가 있는 것을 표현할 수 없음\n",
    "        - ex) 빈티지/레트로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 통계 기반 기법\n",
    "- 말뭉치 : 자연어처리를 염두에 두고 수집된 대량의 텍스트 데이터\n",
    "    - 말뭉치는 사람이 작성한 것이기 때문에 자연어에 대한 사람의 '지식'이 충분히 담겨있음\n",
    "    - 문장을 쓰는 방법, 단어를 선택하는 방법, 단어의 의미 등 사람이 알고있는 자연어에 대한 지식이 포함되어 있음\n",
    "- 시소러스와는 다르게 특징을 자동으로 추출하는 것을 목표로 함\n",
    "- *텍스트 데이터에 대한 추가정보(각 단어의 품사)가 포함되는 경우도 있음*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 파이썬으로 말뭉치 전처리하기\n",
    "- 말뭉치를 단어로 분할하고 그 분할된 단어들을 단어 ID목록으로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\" # 말뭉치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you say goodbye and i say hello  .\n"
     ]
    }
   ],
   "source": [
    "import re # 정규 표현식\n",
    "text = text.lower()\n",
    "# 공백을 기준으로 단어 토큰화(단어 분리)\n",
    "# . 를 고려하여 \".\" -> \" .\"로 변환(공백.)\n",
    "text = text.replace(\".\", \" .\")\n",
    "print(text)\n",
    "\n",
    "words = re.split(\"\\s+\", text) # 공백을 기준으로 분류(공백이 연속되어 있다면 이를 하나로 봄)\n",
    "# +를 붙이지 않으면 매 공백을 기준으로 분류하게 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "RegexpTokenizer(\"\\s+\", gaps=True).tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 대응하는 ID로 짝지어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {} # 단어에서 id로 변환\n",
    "id_to_word = {} # id에서 단어로 변환\n",
    "\n",
    "for word in words:\n",
    "    idx = 0\n",
    "    if word not in word_to_id.keys(): # id가 아직 부여되지 않은 단어라면\n",
    "        new_id = len(word_to_id) # 새로 추가될 id\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_id)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 함수\n",
    "import numpy as np\n",
    "def preprocess(text):\n",
    "    text = text.lower() # 소문자로 변환\n",
    "    text = text.replace(\".\",\" .\") # 공백을 포함해서 .\n",
    "    words = re.split(\"\\s+\", text) # 공백을 기준으로 분류\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, word_to_id, id_to_word = preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 단어의 분산 표현\n",
    "- 단어의 분산 표현 : '단어의 의미'를 정확하게 파악할 수 있는 벡터 표현\n",
    "    - 색에서의 RGB는 3차원 벡터로 색을 정확하게 표현함  \n",
    "- 단어를 벡터로 표현하는 연구의 기초 : 분포가설\n",
    "- 방법\n",
    "    1. 통계 기반 기법 : 어떤 단어에 주목했을 때, 그 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 분포 가설\n",
    "- **분포가설 : 단어의 의미는 주변 단어에 의해 형성**\n",
    "    - 단어 자체에는 의미가 없고, 단어가 사용되는 맥락이 의미를 형성\n",
    "    - 의미가 같은 단어들은 같은 맥락에서 더 많이 등장\n",
    "    - ex) I drink wine / i drink beer -> drink는 음료 주변이라는 맥락에서 많이 사용됨\n",
    "    - ex) i guzzle beer / i guzzle wine -> drink와 비슷한 맥락에서 guzzle이 사용됨 -> 의미가 비슷함을 예상할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 맥락 : 특정 단어를 중심에 둔 그 주변 단어\n",
    "- 윈도우 크기 : 맥락의 크기(주변 단어를 몇 개나 포함할지)\n",
    "    - 윈도우 크기 1 -> 좌우 한단어씩\n",
    "    - 윈도우 크기 2 -> 좌우 두단어씩\n",
    "    - 상황에 따라서 왼단어만/오른단어만/문장의 시작과 끝을 고려할 수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 동시발생 행렬(co-occurrence matrix)\n",
    "- 통계기반기법에 의해서 형성되는 행렬\n",
    "- 특정 단어를 중심에 두고 윈도우 크기에 따라서 맥락 형성, 맥락에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하는 방법\n",
    "    - 특정 단어는 중복 없이 -> word_to_id의 key들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from function.preprocess import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You say goodbye and I say hello.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "print(corpus)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### you say goodbye and i say hello. -> 말뭉치\n",
    "\n",
    "- 의미를 파악해야 할 단어 : you, say, goodbye, and, i, hello, .\n",
    "    - 이 단어의 com -> 6x6 행렬\n",
    "- 윈도우 크기 : 1\n",
    "- you 의 맥락의 벡터\n",
    "\n",
    "|-|you|say|goodbye|and|i|hello|.|\n",
    "|-|-|-|-|-|-|-|-|\n",
    "|you|0|1|0|0|0|0|0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직접 입력\n",
    "C = np.array([\n",
    "    [0,1,0,0,0,0,0],\n",
    "    [1,0,1,0,1,1,0],\n",
    "    [0,1,0,1,0,0,0],\n",
    "    [0,0,1,0,1,0,0],\n",
    "    [0,1,0,1,0,0,0],\n",
    "    [0,1,0,0,0,0,1],\n",
    "    [0,0,0,0,0,0,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동시발생행렬 구하는 함수\n",
    "def create_co_matrix(corpus, vocab_size, window_size = 1):\n",
    "    corpus_size = len(corpus) # 문장의 길이\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32) # 단어의 개수를 통해 com 행렬 형성\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx-i  \n",
    "            right_idx = idx+i\n",
    "            \n",
    "            if left_idx >= 0 : # 왼쪽 경계 확인\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1 # 해당 글자의 왼쪽 문맥 +1\n",
    "            \n",
    "            if right_idx < corpus_size: # 오른쪽 경계 확인\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] +=1 # 해당 글자의 오른쪽 문맥 +1\n",
    "    \n",
    "    return co_matrix        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = create_co_matrix(corpus, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[0 1 0 1 0 0 0]\n",
      "[0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(C[0]) # ID가 0인 단어의 벡터 표현\n",
    "print(C[4]) # ID가 4인 단어의 벡터 표현\n",
    "print(C[word_to_id[\"goodbye\"]]) # 단어가 goodbyd인 것의 벡터 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 벡터 간 유사도\n",
    "- 벡터 사이의 유사도를 측정하는 방법\n",
    "    1. 내적\n",
    "    1. 유클리드 거리\n",
    "    1. 코사인 유사도\n",
    "        - 각 벡터를 정규화(벡터의 크기 = 1)한 후 내적한 값\n",
    "        - 두 벡터가 이루는 각  \n",
    "        $similarity(x,y) = x.y/||x||||y||$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 구하는 함수\n",
    "def cos_similarity(x,y, eps=1e-8):\n",
    "    # 만약 제로 벡터이면 eps 유지 -> 0으로 나누는 것을 막아줌 \n",
    "    # 만약 제로 벡터가 아니면 eps이 반올림되어 다른 값에 흡수 -> 대부분 최종 결과에 영향 X\n",
    "    nx = x/np.sqrt(np.sum(x**2)+eps)\n",
    "    ny = y/np.sqrt(np.sum(y**2)+eps)\n",
    "    \n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from function.cos_similarity import cos_similarity \n",
    "from function.create_co_matrix import create_co_matrix\n",
    "from function.preprocess import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id) # co-matrix의 size\n",
    "C = create_co_matrix(corpus, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067758832467\n"
     ]
    }
   ],
   "source": [
    "c0 = C[word_to_id[\"you\"]]\n",
    "c1 = C[word_to_id[\"i\"]]\n",
    "print(cos_similarity(c0, c1)) # you와 i의 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 유사 단어의 랭킹 표시\n",
    "- 어떤 단어가 검색어로 주어지면, 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수\n",
    "- 인자\n",
    "    - query\n",
    "    - word_to_id\n",
    "    - id_to_word\n",
    "    - word_matrix\n",
    "    - top=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도가 높은 단어를 구하는 함수 \n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    # 검색어를 꺼냄 \n",
    "    if query not in word_to_id:\n",
    "        print(\"%s(을)를 찾을 수 없습니다.\" %query)\n",
    "        return\n",
    "    \n",
    "    print(\"\\n[query]\"+query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size) # vocab에 저장되어 있는 각 단어와의 유사도를 담을 배열\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec) # 자기 자신과의 듀사도도 계산되어 있음\n",
    "    \n",
    "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1*similarity).argsort(): # 유사도가 큰 것 부터 index 반환\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(\"%s: %s\" %(id_to_word[i], similarity[i]))\n",
    "        \n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0]\n",
      "[0 2 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([100, -20, 2])\n",
    "print(x.argsort()) # 오름차순 index\n",
    "print((-x).argsort()) # 내림차순 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query]you\n",
      "goodbye: 0.7071067758832467\n",
      "i: 0.7071067758832467\n",
      "hello: 0.7071067758832467\n",
      "say: 0.0\n",
      "and: 0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from function.cos_similarity import cos_similarity \n",
    "from function.create_co_matrix import create_co_matrix\n",
    "from function.preprocess import preprocess\n",
    "from function.most_similar import most_similar\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar(\"you\", word_to_id, id_to_word, C, top=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
